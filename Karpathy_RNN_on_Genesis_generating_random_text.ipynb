{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Karpathy RNN on Genesis -- generating random text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drwitt/NLP_IDS_790/blob/master/Karpathy_RNN_on_Genesis_generating_random_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e46UywYM7Dt",
        "colab_type": "code",
        "outputId": "b65106d6-2847-4c6e-a16d-028b8aa25f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "Updated in March 2018; Last updated in Aug 2019 by wz \n",
        "\n",
        "@author: A.Karpathy\n",
        "@editor: WZ\n",
        "'''\n",
        "\"\"\"\n",
        "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
        "BSD License\n",
        "\n",
        "\n",
        "You might need to run as well: \n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "to get the NLTK data \n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMinimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\\nBSD License\\n\\n\\nYou might need also\\nimport nltk\\nnltk.download()\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT2NQ7DIOhhD",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the easiest way to work with your data is to mount your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN8N0FUQNIf-",
        "colab_type": "code",
        "outputId": "a0dec943-84ea-4ad0-cbca-2e28d930c2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPzr0vrOa4v",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOSg3oaeM7D5",
        "colab_type": "code",
        "outputId": "df79e5e2-c119-40f9-d807-c9e3c8455987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "#inputtxt='C:\\\\Anaconda3\\\\nltk_data\\\\corpora\\\\genesis\\\\english-kjv.txt'\n",
        "\n",
        "inputtxt='/content/drive/My Drive/0DATA4Experiments/%NLTK/english-kjv.txt'\n",
        "\n",
        "print(inputtxt[0:500])\n",
        "# data I/O"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/0DATA4Experiments/%NLTK/english-kjv.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d0dW4L2M7D-",
        "colab_type": "code",
        "outputId": "9f251377-3748-483b-8628-e3cf6919cc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data = open(inputtxt, 'r').read() # should be simple plain text file\n",
        "print(data[0:100])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In the beginning God created the heaven and the earth.\n",
            "And the earth was without form, and void; and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjIzjCkuQm3W",
        "colab_type": "code",
        "outputId": "a4b6b9ba-b955-42da-8f62-2e6b8a8ca09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "chars = list(set(data))\n",
        "print('chars', chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars ['(', 'G', 'c', 'j', 'q', 'e', '\\n', 'n', 'v', 'k', 'S', ':', 'F', 'b', 'O', 'o', 'W', 'm', 'h', 'L', 'T', 'J', ';', 'y', 't', 'N', 'w', 'E', 'C', ' ', 'M', ')', 'U', '.', 'r', 'R', 'f', 'Y', \"'\", 'u', '!', 'i', '?', 'z', 'd', 'Z', ',', 'l', 'I', 'p', 'B', 'K', 'H', 's', 'A', 'g', 'x', 'P', 'a', 'D']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h798PIwLM7EF",
        "colab_type": "code",
        "outputId": "d8cc3a12-a6f4-4bc4-c0ea-d1c7c01270d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d characters, %d unique.' % (data_size, vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 195515 characters, 60 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfgiCCWM7EL",
        "colab_type": "code",
        "outputId": "3c2fd957-faa4-4220-8226-27b2f61773e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "print(char_to_ix)\n",
        "\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "print(ix_to_char)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'(': 0, 'G': 1, 'c': 2, 'j': 3, 'q': 4, 'e': 5, '\\n': 6, 'n': 7, 'v': 8, 'k': 9, 'S': 10, ':': 11, 'F': 12, 'b': 13, 'O': 14, 'o': 15, 'W': 16, 'm': 17, 'h': 18, 'L': 19, 'T': 20, 'J': 21, ';': 22, 'y': 23, 't': 24, 'N': 25, 'w': 26, 'E': 27, 'C': 28, ' ': 29, 'M': 30, ')': 31, 'U': 32, '.': 33, 'r': 34, 'R': 35, 'f': 36, 'Y': 37, \"'\": 38, 'u': 39, '!': 40, 'i': 41, '?': 42, 'z': 43, 'd': 44, 'Z': 45, ',': 46, 'l': 47, 'I': 48, 'p': 49, 'B': 50, 'K': 51, 'H': 52, 's': 53, 'A': 54, 'g': 55, 'x': 56, 'P': 57, 'a': 58, 'D': 59}\n",
            "{0: '(', 1: 'G', 2: 'c', 3: 'j', 4: 'q', 5: 'e', 6: '\\n', 7: 'n', 8: 'v', 9: 'k', 10: 'S', 11: ':', 12: 'F', 13: 'b', 14: 'O', 15: 'o', 16: 'W', 17: 'm', 18: 'h', 19: 'L', 20: 'T', 21: 'J', 22: ';', 23: 'y', 24: 't', 25: 'N', 26: 'w', 27: 'E', 28: 'C', 29: ' ', 30: 'M', 31: ')', 32: 'U', 33: '.', 34: 'r', 35: 'R', 36: 'f', 37: 'Y', 38: \"'\", 39: 'u', 40: '!', 41: 'i', 42: '?', 43: 'z', 44: 'd', 45: 'Z', 46: ',', 47: 'l', 48: 'I', 49: 'p', 50: 'B', 51: 'K', 52: 'H', 53: 's', 54: 'A', 55: 'g', 56: 'x', 57: 'P', 58: 'a', 59: 'D'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9H874JPM7ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEaK1rycM7EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWrrn0XGM7Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  inputs,targets are both list of integers.\n",
        "  hprev is Hx1 array of initial hidden state\n",
        "  returns the loss, gradients on model parameters, and last hidden state\n",
        "  \"\"\"\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "  # forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "  # backward pass: compute gradients going backwards\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUr_oyKcM7En",
        "colab_type": "raw"
      },
      "source": [
        "\"\"\"   sample a sequence of integers from the model \n",
        "h is memory state, seed_ix is seed letter for first time step \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKvEzxYxM7Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h, seed_ix, n):\n",
        "   x = np.zeros((vocab_size, 1))\n",
        "   x[seed_ix] = 1\n",
        "   ixes = []\n",
        "   for t in range(n):\n",
        "      h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "      y = np.dot(Why, h) + by\n",
        "      p = np.exp(y) / np.sum(np.exp(y))\n",
        "      ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "      x = np.zeros((vocab_size, 1))\n",
        "      x[ix] = 1\n",
        "      ixes.append(ix) \n",
        "   return ixes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWaRoJ4M7Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHftp12EWm2s",
        "colab_type": "text"
      },
      "source": [
        "### Please don't run it for 12h on colab \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ1E5i41M7E4",
        "colab_type": "code",
        "outputId": "c8f5ad6a-be8b-47a6-d1a9-78a41598ad7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while True:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if p+seq_length+1 >= len(data) or n == 0: \n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    p = 0 # go from start of data\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # sample from the model now and then\n",
        "  if n % 10000 == 0:  #was 1000\n",
        "    sample_ix = sample(hprev, inputs[0], 200)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print( '----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 10000 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress #was 1000\n",
        "  \n",
        "  # perform parameter update with Adagrad\n",
        "  '''wz\n",
        "  Returns an iterator of tuples, where the i-th tuple contains the i-th element from each \n",
        "  of the argument sequences or iterables. \n",
        "  The iterator stops when the shortest input iterable is exhausted. \n",
        "  With a single iterable argument, it returns an iterator of 1-tuples.\n",
        "  '''\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
        "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_length # move data pointer\n",
        "  n += 1 # iteration counter "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " ch said, buen and\n",
            "sons juure to a gare of Sacohus, and daven.\n",
            "And Abram said unto thene of I mone\n",
            "yeazregond gase is rose was\n",
            "flel said the veared sorseing he\n",
            "heven sherddeat,\n",
            "and ot two of Iam Asary  \n",
            "----\n",
            "iter 260000, loss: 37.806608\n",
            "----\n",
            " wyod me bedaul wame a let.\n",
            "And for wive of his sons in they\n",
            "par to Abrahaw them, Seth, Is\n",
            "doiegter.\n",
            "And And be got outh her of the son thou take is it me?\n",
            "And the sonount he.\n",
            "And he\n",
            "ret all dim it is  \n",
            "----\n",
            "iter 270000, loss: 36.442787\n",
            "----\n",
            " her brothirn unto bayins of the earred? And Shen thy vieh kins what shouss,\n",
            "and come up was shall bater's; do up and upraranstwers;\n",
            "and welt\n",
            "Lethre not, and hes sont hows not chich withed a m and outh \n",
            "----\n",
            "iter 280000, loss: 37.488415\n",
            "----\n",
            "  him dreanghaencased unto, and yetw anome bret ney to his broven, spring of oft us olst:\n",
            "and up are blestle the passeds unto his hand\n",
            "ushorn and for Gostory.\n",
            "And Pham, evened unde the mumant as find h \n",
            "----\n",
            "iter 290000, loss: 36.897968\n",
            "----\n",
            " anoiz chiled; and I\n",
            "toke Gersed wever.\n",
            "And Iseanteing befall bedamcest this mace night the heard of the sons me Hellam.\n",
            "And it dought; Hamoch sather God of Abrahamd with me all ixsines toce\n",
            "weir wond  \n",
            "----\n",
            "iter 300000, loss: 37.130867\n",
            "----\n",
            " d we\n",
            "floss\n",
            "he sernand the\n",
            "gave this weteel, born to Abrahaw the rig; Labazred gran after will slalk of Jacob is the ding, I a noret no all danked are that that he all go\n",
            "load agaud cond in the sept wa \n",
            "----\n",
            "iter 310000, loss: 36.700969\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3abca79ec916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print progress #was 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b6e72c13c965>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clip to mitigate exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wsHUK1uM7FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prior run sampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9zgERPQM7FF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "----\n",
        " MlpSYgG)PscAJcCF)maYMxgcsAEMeaG'uG!awALhf\n",
        "nTbnBs'?C?EmdMe''JaLW)fDFH(zHGJdbkGhZNSP:pAWYvaYycGbpo,dx(iMZWUHK\n",
        "h)zLSqproOICzgvLbGo(.a, ?hK?v\n",
        "kDwLoWC;PiecyfmGnE:\n",
        "TLhj),gv\n",
        "F.k,s;\n",
        "cfL(IamKwqnNzihWusAFIROgIu \n",
        "----\n",
        "iter 0, loss: 102.358623\n",
        "----\n",
        " ll cworon of foed wa.\n",
        "c ide wtonAD fanty faJm aol dehibe ,he thee dald y,afas. hil fofe wahan.d; th aindeethet ans mnd eiri heter\n",
        "ind Nveyy\n",
        "falnle vathe thallt, url, of arert spy toat and tohy ge meeo \n",
        "----\n",
        "iter 1000, loss: 80.115990\n",
        "----\n",
        " , I unthedering alr thar iled Gof hing thevyrovend and herth ild, yam hes\n",
        "sorisu.\n",
        "pet w\n",
        "se\n",
        "kat his how le\n",
        "Ardixd I Hhile, ind\n",
        "the the\n",
        "mi goas bime Idind, And mald henling fomay, alaeg doter; wance I f \n",
        "----\n",
        "iter 2000, loss: 65.526007\n",
        "----\n",
        "  in noths of chou'd Gean pavenco gid liy the the he elrs thoins min ox.\n",
        "And the iod go sotoy saaven and ofhim he Noch the ahe on tee in ok thaecem, lhamthe salay.\n",
        "And sava htht thricarshers\n",
        "them thae  \n",
        "----\n",
        "iter 3000, loss: 57.927522\n",
        "----\n",
        " ---\n",
        "  wored all don terver,\n",
        "whot liverwe vift, wuund ban anten.\n",
        "And mad unto if us thy hes in thou unto her, and case shall not a thren\n",
        "sputh unto mazind him Abrahah.\n",
        "And they lill dounst will the king to  \n",
        "----\n",
        "iter 50000, loss: 42.217230\n",
        "----\n",
        " \n",
        "----\n",
        " \n",
        "And the sais\n",
        "Enou food men behold, spour him Hass in to hit fassy in theirs.\n",
        "And whous Phar' shau. And Joreh en the cond goed lose, and in on youre, and it tyough.f fore, and to sast\n",
        "ho, Behold be ao \n",
        "----\n",
        "iter 85000, loss: 40.487233\n",
        "----\n",
        "  king is my said, bucass. And he soed satild youl and the bros on Poweph yeom beforest land in shalssed; as at\n",
        "he notsy be Jacob I alle:\n",
        "Is\n",
        "ald.\n",
        "And rearftour: and they tles wild ald my falleld ha fol \n",
        "----\n",
        "iter 86000, loss: 39.939279\n",
        "----\n",
        "----\n",
        " may in\n",
        "Esaughid servaget Isaach the fapleb\n",
        "the begam, Unar theued and to the pare ase gad anoon bleires not sat at his sons\n",
        "wast sons thee pord hiveh burost sabob him a fie said, breons\n",
        "the eald shirk \n",
        "----\n",
        "iter 125000, loss: 39.021318\n",
        "----\n",
        " ed the mon of the earelvang, of will of God, and intt\n",
        "samal.\n",
        "And this hast they awale, Beho the comed douth, Pather spay madh they Leew, sseanter onding the lowtord terimese: Fearah of upon.\n",
        "The\n",
        "went  \n",
        "----\n",
        "iter 126000, loss: 38.998472\n",
        "----\n",
        "----\n",
        "iter 168000, loss: 38.126692\n",
        "----\n",
        " ee his not then, the seas tom shall said, I with silt, Os naml, when Abraht thy seevantsth, and houcher\n",
        "sthat caso is Abraham's nase teme. butobhes not head came of my\n",
        "are shailf begat kistrer, unto w \n",
        "----\n",
        "iter 169000, loss: 38.216707\n",
        "----\n",
        "pots, reart, and were agired \n",
        "----\n",
        "iter 199000, loss: 37.881385\n",
        "----\n",
        " hen\n",
        "unto the shall brouse,\n",
        "fill a lothered hid with me, dake will bearath all ane wiver is forth brock that eves,\n",
        "and shele upon\n",
        "the graceed vawat have Rachee inger as Adat in to Jacob be agained so o \n",
        "----\n",
        "iter 200000, loss: 37.349877\n",
        "----\n",
        " d his pin\n",
        "dreabinge out\n",
        "of Jadelis, and had up his said, whit I but thele of a feet this in to\n",
        "will of Hasron.\n",
        "And ithedd I dweldy mured's wi livedter he said, Whouseart shall Bechel\n",
        "in the in ano, an \n",
        "----\n",
        "----\n",
        "iter 218000, loss: 37.791892\n",
        "----\n",
        " hy agoin of the forchemebes Esaurn tham neraned hiigst years\n",
        "his owered at shall the brepet perdinge him\n",
        "Esiin remivering, wen, wheirn and the lersin unto the eard\n",
        "they up to thy giver tole offerd off \n",
        "----\n",
        "iter 219000, loss: 37.282680\n",
        "----\n",
        " 's of the king of hie they are\n",
        "not. Sorn me intt of thy cpae undremo.\n",
        "And the\n",
        "youprnas, bort unto\n",
        "the satelerp, and in that the ray, Learth.\n",
        "And the LORDarresh of hered is went LORD dreing madefe thou \n",
        "----\n",
        "iter 220000, loss: 37.477838\n",
        "----\n",
        " ilt said there\n",
        "of wad Abramaen dothy will lad ix the gilt with begai go dimied chanted, and give him upon the meish bach of thounstn his with my may ofly wifl it this sent, as no benceay.\n",
        "And they, an \n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}